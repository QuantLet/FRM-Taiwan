{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.dates as mdates\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler\n",
    "from sklearn.linear_model import LogisticRegression, LassoCV, Lasso\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import roc_auc_score, brier_score_loss\n",
    "from tqdm import tqdm\n",
    "from xgboost import XGBClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler = StandardScaler()\n",
    "\n",
    "VIX = pd.read_csv('../01 Raw Data/Others/VIXTWN.csv', index_col='Date', parse_dates=['Date'])\n",
    "VIX[:] = scaler.fit_transform(VIX.values)\n",
    "\n",
    "MI = pd.read_csv('../01 Raw Data/Others/MI.csv', index_col='Date', parse_dates=['Date'])\n",
    "MI['Recession'] = MI['MI'].apply(lambda x: 1 if x <= 16 else 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "datasets = ['Lambda', 'VIX', 'FRM']\n",
    "features = ['ALL', 'LASSO']\n",
    "companies = ['Cap_63', 'Cap_126', 'Ele_63', 'Ele_126', 'Fin_63', 'Fin_126', 'FinEle_63', 'FinEle_126']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lambda"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = 'Lambda'\n",
    "\n",
    "for feature in features:\n",
    "    companies = ['Cap_63', 'Cap_126', 'Ele_63', 'Ele_126', 'Fin_63', 'Fin_126', 'FinEle_63', 'FinEle_126']\n",
    "    for company in companies:\n",
    "        sector, ws = company.split('_')[0], int(company.split('_')[1])\n",
    "\n",
    "        print(f'{dataset}, Feature = {feature}, {company}')\n",
    "        top = 20\n",
    "        lag_ws = 63\n",
    "        input_path1 = f'../03 Modeling/{company}'\n",
    "        input_path2 = f'../01 Raw Data/{sector}'\n",
    "        output_path = f'{company}'\n",
    "        if not os.path.exists(output_path):\n",
    "            os.makedirs(output_path)\n",
    "\n",
    "        full_lambda = pd.read_csv(f'{input_path1}/full_lambda.csv', index_col='Date', parse_dates=['Date'])\n",
    "        stock_rank = pd.read_csv(f'{input_path2}/Top rank company.csv', index_col='Date', parse_dates=['Date'])\n",
    "        stock_rank = stock_rank.astype(str)\n",
    "        lambda_rank = pd.DataFrame(index=full_lambda.index, columns=range(1, 21))\n",
    "        for date in full_lambda.index:\n",
    "            for col in lambda_rank.columns:\n",
    "                company_id = str(stock_rank.loc[date, str(col)]).split('.')[0]\n",
    "                lambda_rank.loc[date, col] = full_lambda.loc[date, company_id]\n",
    "        scaler = MinMaxScaler()\n",
    "        lambda_rank[:] = scaler.fit_transform(lambda_rank.values)\n",
    "        lambda_rank_L = pd.DataFrame()\n",
    "        for column in lambda_rank:\n",
    "            lagged_cols = {f'{column}_{lag}': lambda_rank.loc[:,column].shift(lag) for lag in range(round(lag_ws+1))}\n",
    "            lagged = pd.concat(lagged_cols, axis=1).dropna().resample('M').last()\n",
    "            lambda_rank_L = pd.concat([lambda_rank_L, lagged], axis=1)\n",
    "        MI_filter = MI.loc[MI.index.to_series().apply(lambda x: pd.Period(x, freq='M')).isin(lambda_rank_L.index.to_period('M').unique())]\n",
    "        X = lambda_rank_L.copy()\n",
    "\n",
    "        y = MI_filter['Recession']\n",
    "\n",
    "        for col in X.columns:\n",
    "            X[col] = pd.to_numeric(X[col], errors='coerce')\n",
    "\n",
    "        cut = 5\n",
    "        n = len(X)\n",
    "\n",
    "        models = {\n",
    "            \"Logistic Regression\": LogisticRegression(max_iter=10000, random_state=42, multi_class='ovr'),\n",
    "            \"XGBoost\": XGBClassifier(eval_metric='mlogloss', random_state=42)\n",
    "        }\n",
    "\n",
    "        results = pd.DataFrame(columns=[\"LR_AUC\", \"LR_Brier\", \"LR_AUC\", \"LR_Brier\", \"XGB_AUC\", \"XGB_Brier\", \"XGB_AUC\", \"XGB_Brier\"])\n",
    "        all_predictions = {model_name: pd.Series([None] * n, index=MI_filter.index) for model_name in models.keys()}\n",
    "\n",
    "        n_samples = X.shape[0]\n",
    "        split_size = n_samples // cut\n",
    "\n",
    "        for i in tqdm(range(cut)):\n",
    "            start_test = i * split_size\n",
    "            end_test = (i + 1) * split_size if i < cut - 1 else n_samples\n",
    "\n",
    "            X_test = X[start_test:end_test]\n",
    "            y_test = y[start_test:end_test]\n",
    "\n",
    "            indices = list(range(0, start_test)) + list(range(end_test, n_samples))\n",
    "            X_train = X.iloc[indices]\n",
    "            y_train = y.iloc[indices]\n",
    "\n",
    "            if feature == 'LASSO':\n",
    "                model = LogisticRegression(penalty='l1', solver='saga', max_iter=100000, random_state=42)\n",
    "                model.fit(X_train, y_train)\n",
    "                selected_features = np.where(model.coef_[0] != 0)[0]\n",
    "                X_train = X_train.iloc[:, selected_features]\n",
    "                X_test = X_test.iloc[:, selected_features]\n",
    "\n",
    "            row_results = []\n",
    "\n",
    "            for name, model in models.items():\n",
    "                model.fit(X_train, y_train)\n",
    "                \n",
    "                y_pred_train = model.predict(X_train)\n",
    "                y_pred_test = model.predict(X_test)\n",
    "                y_pred_proba_train = model.predict_proba(X_train)[:, 1]\n",
    "                y_pred_proba_test = model.predict_proba(X_test)[:, 1]\n",
    "\n",
    "                auc_score_train = roc_auc_score(y_train, y_pred_proba_train)\n",
    "                brier_score_train = brier_score_loss(y_train, y_pred_proba_train)\n",
    "                auc_score_test = roc_auc_score(y_test, y_pred_proba_test)\n",
    "                brier_score_test = brier_score_loss(y_test, y_pred_proba_test)\n",
    "\n",
    "                row_results.extend([round(auc_score_train, 4), round(brier_score_train, 4), round(auc_score_test, 4), round(brier_score_test, 4)])\n",
    "\n",
    "                all_predictions[name].iloc[start_test:end_test] = y_pred_proba_test\n",
    "\n",
    "                fig, ax = plt.subplots(figsize=(20, 4))\n",
    "                color = 'darkgoldenrod' if name == 'Logistic Regression' else 'darkblue'\n",
    "                ax.scatter(MI_filter.index[indices], y_train, label='Actual', color='darkred', marker='s', s=14)\n",
    "                ax.plot(MI_filter.index[indices], y_pred_proba_train, label=f'Predicted by {name}', linewidth=4, color=color, alpha=0.2)\n",
    "                count = 0\n",
    "                for idx in indices:\n",
    "                    if y_train.loc[MI_filter.index[idx]] == 1 and y_pred_proba_train[count] < 0.5:\n",
    "                        ax.scatter(MI_filter.index[idx], y_pred_proba_train[count], color='darkgreen', marker='x', s=50, lw=2.5, alpha=1)\n",
    "                    elif y_train.loc[MI_filter.index[idx]] == 0 and y_pred_proba_train[count] > 0.5:\n",
    "                        ax.scatter(MI_filter.index[idx], y_pred_proba_train[count], color='darkgreen', marker='^', s=40, lw=1, alpha=1)\n",
    "                    else:\n",
    "                        ax.scatter(MI_filter.index[idx], y_pred_proba_train[count], color=color, alpha=1)\n",
    "                    count += 1\n",
    "                ax.axhline(y=0.5, color='k', linestyle='--', alpha=0.4)\n",
    "                ax.set_xlim(y.index[0], y.index[-1])\n",
    "                ax.set_ylim(-0.1, 1.1)\n",
    "                ax.tick_params(axis='y', labelsize=8, width=2)\n",
    "                ax.tick_params(axis='x', labelsize=8, width=2)\n",
    "                ax.xaxis.set_major_locator(mdates.YearLocator())\n",
    "                ax.xaxis.set_major_formatter(mdates.DateFormatter('%Y'))\n",
    "                plt.tight_layout()\n",
    "                plt.savefig(f'{output_path}/IN {dataset} {feature} {name} F{i+1}.png', dpi=144, transparent=True)\n",
    "                plt.close()\n",
    "\n",
    "            results.loc[i] = row_results\n",
    "\n",
    "        results.columns = [\"LR_AUC\", \"LR_Brier\", \"LR_AUC\", \"LR_Brier\", \"XGB_AUC\", \"XGB_Brier\", \"XGB_AUC\", \"XGB_Brier\"]\n",
    "\n",
    "        average_results = results.mean().to_frame().T\n",
    "        average_results.index = [\"-\"]\n",
    "\n",
    "        print(results.round(4))\n",
    "        print(average_results.round(4))\n",
    "        print()\n",
    "\n",
    "        for name in models.keys():\n",
    "            plt.figure(figsize=(20, 4))\n",
    "            color = 'darkgoldenrod' if name == 'Logistic Regression' else 'darkblue'\n",
    "            plt.scatter(MI_filter.index, y, label='Actual', color='darkred', marker='s', s=14)\n",
    "            plt.plot(MI_filter.index, all_predictions[name], label=f'Predicted by {name}', linewidth=4, color=color, alpha=0.2)\n",
    "            for i in range(len(all_predictions[name])):\n",
    "                if y.iloc[i] == 1 and all_predictions[name].values[i] < 0.5:\n",
    "                    plt.scatter(all_predictions[name].index[i], all_predictions[name].values[i], color='darkgreen', marker='x', s=50, lw=2.5, alpha=1)\n",
    "                elif y.iloc[i] == 0 and all_predictions[name].values[i] > 0.5:\n",
    "                    plt.scatter(all_predictions[name].index[i], all_predictions[name].values[i], color='darkgreen', marker='^', s=40, lw=1, alpha=1)\n",
    "                else:\n",
    "                    plt.scatter(all_predictions[name].index[i], all_predictions[name].values[i], color=color, alpha=1)\n",
    "\n",
    "            plt.axhline(y=0.5, color='k', linestyle='--', alpha=0.4)\n",
    "            plt.ylim(-0.1, 1.1)\n",
    "            plt.yticks(fontsize=14, fontweight='heavy')\n",
    "            plt.xticks(fontsize=14, fontweight='heavy')\n",
    "            plt.gca().xaxis.set_major_locator(mdates.YearLocator())\n",
    "            plt.gca().xaxis.set_major_formatter(mdates.DateFormatter('%Y'))\n",
    "            plt.tight_layout()\n",
    "            plt.savefig(f'{output_path}/OUT {dataset} {feature} {name}.png', dpi=144, transparent=True)\n",
    "            plt.close()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# FRM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = 'FRM'\n",
    "\n",
    "for feature in features:\n",
    "    companies = ['Cap_63', 'Cap_126', 'Ele_63', 'Ele_126', 'Fin_63', 'Fin_126', 'FinEle_63', 'FinEle_126']\n",
    "    for company in companies:\n",
    "        sector, ws = company.split('_')[0], int(company.split('_')[1])\n",
    "\n",
    "        print(f'{company}, Feature = {feature}')\n",
    "        top = 20\n",
    "        lag_ws = 63\n",
    "        input_path1 = f'../03 Modeling/{company}'\n",
    "        input_path2 = f'../01 Raw Data/{sector}'\n",
    "        output_path = f'{company}'\n",
    "        if not os.path.exists(output_path):\n",
    "            os.makedirs(output_path)\n",
    "\n",
    "        full_lambda = pd.read_csv(f'{input_path1}/full_lambda.csv', index_col='Date', parse_dates=['Date'])\n",
    "        FRM = full_lambda.mean(axis=1) * full_lambda.shape[1] / top\n",
    "        lagged_cols = {f'FRM_lag_{lag}': FRM.shift(lag) for lag in range(round(lag_ws+1))}\n",
    "        FRM_L = pd.concat(lagged_cols, axis=1).dropna().resample('M').last()\n",
    "        FRM_L = FRM_L.dropna()\n",
    "        FRM_L = FRM_L.resample('M').last()\n",
    "        MI_filter = MI.loc[MI.index.to_series().apply(lambda x: pd.Period(x, freq='M')).isin(FRM_L.index.to_period('M').unique())]\n",
    "        \n",
    "        X = FRM_L.copy()\n",
    "        y = MI_filter['Recession']\n",
    "\n",
    "        for col in X.columns:\n",
    "            X[col] = pd.to_numeric(X[col], errors='coerce')\n",
    "\n",
    "        cut = 5\n",
    "        n = len(X)\n",
    "\n",
    "        models = {\n",
    "            \"Logistic Regression\": LogisticRegression(max_iter=10000, random_state=42, multi_class='ovr'),\n",
    "            \"XGBoost\": XGBClassifier(eval_metric='mlogloss', random_state=42)\n",
    "        }\n",
    "\n",
    "        results = pd.DataFrame(columns=[\"LR_AUC\", \"LR_Brier\", \"LR_AUC\", \"LR_Brier\", \"XGB_AUC\", \"XGB_Brier\", \"XGB_AUC\", \"XGB_Brier\"])\n",
    "        all_predictions = {model_name: pd.Series([None] * n, index=MI_filter.index) for model_name in models.keys()}\n",
    "\n",
    "        n_samples = X.shape[0]\n",
    "        split_size = n_samples // cut\n",
    "\n",
    "        for i in tqdm(range(cut)):\n",
    "            start_test = i * split_size\n",
    "            end_test = (i + 1) * split_size if i < cut - 1 else n_samples\n",
    "\n",
    "            X_test = X[start_test:end_test]\n",
    "            y_test = y[start_test:end_test]\n",
    "\n",
    "            indices = list(range(0, start_test)) + list(range(end_test, n_samples))\n",
    "            X_train = X.iloc[indices]\n",
    "            y_train = y.iloc[indices]\n",
    "\n",
    "            if feature == 'LASSO':\n",
    "                model = LogisticRegression(penalty='l1', solver='saga', max_iter=100000, random_state=42)\n",
    "                model.fit(X_train, y_train)\n",
    "                selected_features = np.where(model.coef_[0] != 0)[0]\n",
    "                X_train = X_train.iloc[:, selected_features]\n",
    "                X_test = X_test.iloc[:, selected_features]\n",
    "\n",
    "            row_results = []\n",
    "\n",
    "            for name, model in models.items():\n",
    "                model.fit(X_train, y_train)\n",
    "                \n",
    "                y_pred_train = model.predict(X_train)\n",
    "                y_pred_test = model.predict(X_test)\n",
    "                y_pred_proba_train = model.predict_proba(X_train)[:, 1]\n",
    "                y_pred_proba_test = model.predict_proba(X_test)[:, 1]\n",
    "\n",
    "                auc_score_train = roc_auc_score(y_train, y_pred_proba_train)\n",
    "                brier_score_train = brier_score_loss(y_train, y_pred_proba_train)\n",
    "                auc_score_test = roc_auc_score(y_test, y_pred_proba_test)\n",
    "                brier_score_test = brier_score_loss(y_test, y_pred_proba_test)\n",
    "\n",
    "                row_results.extend([round(auc_score_train, 4), round(brier_score_train, 4), round(auc_score_test, 4), round(brier_score_test, 4)])\n",
    "\n",
    "                all_predictions[name].iloc[start_test:end_test] = y_pred_proba_test\n",
    "\n",
    "                fig, ax = plt.subplots(figsize=(20, 4))\n",
    "                color = 'darkgoldenrod' if name == 'Logistic Regression' else 'darkblue'\n",
    "                ax.scatter(MI_filter.index[indices], y_train, label='Actual', color='darkred', marker='s', s=14)\n",
    "                ax.plot(MI_filter.index[indices], y_pred_proba_train, label=f'Predicted by {name}', linewidth=4, color=color, alpha=0.2)\n",
    "                count = 0\n",
    "                for idx in indices:\n",
    "                    if y_train.loc[MI_filter.index[idx]] == 1 and y_pred_proba_train[count] < 0.5:\n",
    "                        ax.scatter(MI_filter.index[idx], y_pred_proba_train[count], color='darkgreen', marker='x', s=50, lw=2.5, alpha=1)\n",
    "                    elif y_train.loc[MI_filter.index[idx]] == 0 and y_pred_proba_train[count] > 0.5:\n",
    "                        ax.scatter(MI_filter.index[idx], y_pred_proba_train[count], color='darkgreen', marker='^', s=40, lw=1, alpha=1)\n",
    "                    else:\n",
    "                        ax.scatter(MI_filter.index[idx], y_pred_proba_train[count], color=color, alpha=1)\n",
    "                    count += 1\n",
    "                ax.axhline(y=0.5, color='k', linestyle='--', alpha=0.4)\n",
    "                ax.set_xlim(y.index[0], y.index[-1])\n",
    "                ax.set_ylim(-0.1, 1.1)\n",
    "                ax.tick_params(axis='y', labelsize=8, width=2)\n",
    "                ax.tick_params(axis='x', labelsize=8, width=2)\n",
    "                ax.xaxis.set_major_locator(mdates.YearLocator())\n",
    "                ax.xaxis.set_major_formatter(mdates.DateFormatter('%Y'))\n",
    "                plt.tight_layout()\n",
    "                plt.savefig(f'{output_path}/IN {dataset} {feature} {name} F{i+1}.png', dpi=144, transparent=True)\n",
    "                plt.close()\n",
    "\n",
    "            results.loc[i] = row_results\n",
    "\n",
    "        results.columns = [\"LR_AUC\", \"LR_Brier\", \"LR_AUC\", \"LR_Brier\", \"XGB_AUC\", \"XGB_Brier\", \"XGB_AUC\", \"XGB_Brier\"]\n",
    "\n",
    "        average_results = results.mean().to_frame().T\n",
    "        average_results.index = [\"-\"]\n",
    "\n",
    "        print(results.round(4))\n",
    "        print(average_results.round(4))\n",
    "        print()\n",
    "\n",
    "        for name in models.keys():\n",
    "            plt.figure(figsize=(20, 4))\n",
    "            color = 'darkgoldenrod' if name == 'Logistic Regression' else 'darkblue'\n",
    "            plt.scatter(MI_filter.index, y, label='Actual', color='darkred', marker='s', s=14)\n",
    "            plt.plot(MI_filter.index, all_predictions[name], label=f'Predicted by {name}', linewidth=4, color=color, alpha=0.2)\n",
    "            for i in range(len(all_predictions[name])):\n",
    "                if y.iloc[i] == 1 and all_predictions[name].values[i] < 0.5:\n",
    "                    plt.scatter(all_predictions[name].index[i], all_predictions[name].values[i], color='darkgreen', marker='x', s=50, lw=2.5, alpha=1)\n",
    "                elif y.iloc[i] == 0 and all_predictions[name].values[i] > 0.5:\n",
    "                    plt.scatter(all_predictions[name].index[i], all_predictions[name].values[i], color='darkgreen', marker='^', s=40, lw=1, alpha=1)\n",
    "                else:\n",
    "                    plt.scatter(all_predictions[name].index[i], all_predictions[name].values[i], color=color, alpha=1)\n",
    "\n",
    "            plt.axhline(y=0.5, color='k', linestyle='--', alpha=0.4)\n",
    "            plt.ylim(-0.1, 1.1)\n",
    "            plt.yticks(fontsize=14, fontweight='heavy')\n",
    "            plt.xticks(fontsize=14, fontweight='heavy')\n",
    "            plt.gca().xaxis.set_major_locator(mdates.YearLocator())\n",
    "            plt.gca().xaxis.set_major_formatter(mdates.DateFormatter('%Y'))\n",
    "            plt.tight_layout()\n",
    "            plt.savefig(f'{output_path}/OUT {dataset} {feature} {name}.png', dpi=144, transparent=True)\n",
    "            plt.close()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# VIX"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = 'VIX'\n",
    "\n",
    "for feature in features:\n",
    "    for lag_ws in [63, 126, 189]:\n",
    "        print(f'{dataset}, Feature = {feature}')\n",
    "        top = 20\n",
    "        input_path1 = f'../03 Modeling/{company}'\n",
    "        input_path2 = f'../01 Raw Data/{sector}'\n",
    "        output_path = 'VIX'\n",
    "        if not os.path.exists(output_path):\n",
    "            os.makedirs(output_path)\n",
    "\n",
    "        lagged_cols = {f'VIX_lag_{lag}': VIX['VIXTWN'].shift(lag) for lag in range(round(lag_ws+1))}\n",
    "        VIX_L = pd.concat(lagged_cols, axis=1).dropna().resample('M').last()\n",
    "        VIX_L = VIX_L.dropna()\n",
    "        VIX_L = VIX_L.resample('M').last()\n",
    "        MI_filter = MI.loc[MI.index.to_series().apply(lambda x: pd.Period(x, freq='M')).isin(VIX_L.index.to_period('M').unique())]\n",
    "        \n",
    "        X = VIX_L.copy()\n",
    "        y = MI_filter['Recession']\n",
    "\n",
    "        for col in X.columns:\n",
    "            X[col] = pd.to_numeric(X[col], errors='coerce')\n",
    "\n",
    "        cut = 5\n",
    "        n = len(X)\n",
    "\n",
    "        models = {\n",
    "            \"Logistic Regression\": LogisticRegression(max_iter=10000, random_state=42, multi_class='ovr'),\n",
    "            \"XGBoost\": XGBClassifier(eval_metric='mlogloss', random_state=42)\n",
    "        }\n",
    "\n",
    "        results = pd.DataFrame(columns=[\"LR_AUC\", \"LR_Brier\", \"LR_AUC\", \"LR_Brier\", \"XGB_AUC\", \"XGB_Brier\", \"XGB_AUC\", \"XGB_Brier\"])\n",
    "        all_predictions = {model_name: pd.Series([None] * n, index=MI_filter.index) for model_name in models.keys()}\n",
    "\n",
    "        n_samples = X.shape[0]\n",
    "        split_size = n_samples // cut\n",
    "\n",
    "        for i in tqdm(range(cut)):\n",
    "            start_test = i * split_size\n",
    "            end_test = (i + 1) * split_size if i < cut - 1 else n_samples\n",
    "\n",
    "            X_test = X[start_test:end_test]\n",
    "            y_test = y[start_test:end_test]\n",
    "\n",
    "            indices = list(range(0, start_test)) + list(range(end_test, n_samples))\n",
    "            X_train = X.iloc[indices]\n",
    "            y_train = y.iloc[indices]\n",
    "\n",
    "            if feature == 'LASSO':\n",
    "                model = LogisticRegression(penalty='l1', solver='saga', max_iter=100000, random_state=42)\n",
    "                model.fit(X_train, y_train)\n",
    "                selected_features = np.where(model.coef_[0] != 0)[0]\n",
    "                X_train = X_train.iloc[:, selected_features]\n",
    "                X_test = X_test.iloc[:, selected_features]\n",
    "\n",
    "            row_results = []\n",
    "\n",
    "            for name, model in models.items():\n",
    "                model.fit(X_train, y_train)\n",
    "                \n",
    "                y_pred_train = model.predict(X_train)\n",
    "                y_pred_test = model.predict(X_test)\n",
    "                y_pred_proba_train = model.predict_proba(X_train)[:, 1]\n",
    "                y_pred_proba_test = model.predict_proba(X_test)[:, 1]\n",
    "\n",
    "                auc_score_train = roc_auc_score(y_train, y_pred_proba_train)\n",
    "                brier_score_train = brier_score_loss(y_train, y_pred_proba_train)\n",
    "                auc_score_test = roc_auc_score(y_test, y_pred_proba_test)\n",
    "                brier_score_test = brier_score_loss(y_test, y_pred_proba_test)\n",
    "\n",
    "                row_results.extend([round(auc_score_train, 4), round(brier_score_train, 4), round(auc_score_test, 4), round(brier_score_test, 4)])\n",
    "\n",
    "                all_predictions[name].iloc[start_test:end_test] = y_pred_proba_test\n",
    "\n",
    "                fig, ax = plt.subplots(figsize=(20, 4))\n",
    "                color = 'darkgoldenrod' if name == 'Logistic Regression' else 'darkblue'\n",
    "                ax.scatter(MI_filter.index[indices], y_train, label='Actual', color='darkred', marker='s', s=14)\n",
    "                ax.plot(MI_filter.index[indices], y_pred_proba_train, label=f'Predicted by {name}', linewidth=4, color=color, alpha=0.2)\n",
    "                count = 0\n",
    "                for idx in indices:\n",
    "                    if y_train.loc[MI_filter.index[idx]] == 1 and y_pred_proba_train[count] < 0.5:\n",
    "                        ax.scatter(MI_filter.index[idx], y_pred_proba_train[count], color='darkgreen', marker='x', s=50, lw=2.5, alpha=1)\n",
    "                    elif y_train.loc[MI_filter.index[idx]] == 0 and y_pred_proba_train[count] > 0.5:\n",
    "                        ax.scatter(MI_filter.index[idx], y_pred_proba_train[count], color='darkgreen', marker='^', s=40, lw=1, alpha=1)\n",
    "                    else:\n",
    "                        ax.scatter(MI_filter.index[idx], y_pred_proba_train[count], color=color, alpha=1)\n",
    "                    count += 1\n",
    "                ax.axhline(y=0.5, color='k', linestyle='--', alpha=0.4)\n",
    "                ax.set_xlim(y.index[0], y.index[-1])\n",
    "                ax.set_ylim(-0.1, 1.1)\n",
    "                ax.tick_params(axis='y', labelsize=8, width=2)\n",
    "                ax.tick_params(axis='x', labelsize=8, width=2)\n",
    "                ax.xaxis.set_major_locator(mdates.YearLocator())\n",
    "                ax.xaxis.set_major_formatter(mdates.DateFormatter('%Y'))\n",
    "                plt.tight_layout()\n",
    "                plt.savefig(f'{output_path}/IN {dataset} {lag_ws} {feature} {name} F{i+1}.png', dpi=144, transparent=True)\n",
    "                plt.close()\n",
    "\n",
    "            results.loc[i] = row_results\n",
    "\n",
    "        results.columns = [\"LR_AUC\", \"LR_Brier\", \"LR_AUC\", \"LR_Brier\", \"XGB_AUC\", \"XGB_Brier\", \"XGB_AUC\", \"XGB_Brier\"]\n",
    "\n",
    "        average_results = results.mean().to_frame().T\n",
    "        average_results.index = [\"-\"]\n",
    "\n",
    "        print(results.round(4))\n",
    "        print(average_results.round(4))\n",
    "        print()\n",
    "\n",
    "        for name in models.keys():\n",
    "            plt.figure(figsize=(20, 4))\n",
    "            color = 'darkgoldenrod' if name == 'Logistic Regression' else 'darkblue'\n",
    "            plt.scatter(MI_filter.index, y, label='Actual', color='darkred', marker='s', s=14)\n",
    "            plt.plot(MI_filter.index, all_predictions[name], label=f'Predicted by {name}', linewidth=4, color=color, alpha=0.2)\n",
    "            for i in range(len(all_predictions[name])):\n",
    "                if y.iloc[i] == 1 and all_predictions[name].values[i] < 0.5:\n",
    "                    plt.scatter(all_predictions[name].index[i], all_predictions[name].values[i], color='darkgreen', marker='x', s=50, lw=2.5, alpha=1)\n",
    "                elif y.iloc[i] == 0 and all_predictions[name].values[i] > 0.5:\n",
    "                    plt.scatter(all_predictions[name].index[i], all_predictions[name].values[i], color='darkgreen', marker='^', s=40, lw=1, alpha=1)\n",
    "                else:\n",
    "                    plt.scatter(all_predictions[name].index[i], all_predictions[name].values[i], color=color, alpha=1)\n",
    "\n",
    "            plt.axhline(y=0.5, color='k', linestyle='--', alpha=0.4)\n",
    "            plt.ylim(-0.1, 1.1)\n",
    "            plt.yticks(fontsize=14, fontweight='heavy')\n",
    "            plt.xticks(fontsize=14, fontweight='heavy')\n",
    "            plt.gca().xaxis.set_major_locator(mdates.YearLocator())\n",
    "            plt.gca().xaxis.set_major_formatter(mdates.DateFormatter('%Y'))\n",
    "            plt.tight_layout()\n",
    "            plt.savefig(f'{output_path}/OUT {dataset} {lag_ws} {feature} {name}.png', dpi=144, transparent=True)\n",
    "            plt.close()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lambdas, without Feature selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature = 'ALL'\n",
    "\n",
    "companies = ['Cap_63', 'Cap_126', 'Ele_63', 'Ele_126', 'Fin_63', 'Fin_126', 'FinEle_63', 'FinEle_126']\n",
    "company = 'Cap_63'\n",
    "for company in companies:\n",
    "    sector, ws = company.split('_')[0], int(company.split('_')[1])\n",
    "\n",
    "    print(company)\n",
    "    top = 20\n",
    "    lag_ws = 63\n",
    "    input_path1 = f'../03 Modeling/{company}'\n",
    "    input_path2 = f'../01 Raw Data/{sector}'\n",
    "    output_path = f'{company}'\n",
    "    if not os.path.exists(output_path):\n",
    "        os.makedirs(output_path)\n",
    "\n",
    "    full_lambda = pd.read_csv(f'{input_path1}/full_lambda.csv', index_col='Date', parse_dates=['Date'])\n",
    "    stock_rank = pd.read_csv(f'{input_path2}/Top rank company.csv', index_col='Date', parse_dates=['Date'])\n",
    "    stock_rank = stock_rank.astype(str)\n",
    "\n",
    "    lambda_rank = pd.DataFrame(index=full_lambda.index, columns=range(1, 21))\n",
    "    for date in full_lambda.index:\n",
    "        for col in lambda_rank.columns:\n",
    "            company_id = str(stock_rank.loc[date, str(col)]).split('.')[0]\n",
    "            lambda_rank.loc[date, col] = full_lambda.loc[date, company_id]\n",
    "    scaler = MinMaxScaler()\n",
    "    lambda_rank[:] = scaler.fit_transform(lambda_rank.values)\n",
    "    lambda_rank_L = pd.DataFrame()\n",
    "    for column in lambda_rank:\n",
    "        lagged_cols = {f'{column}_{lag}': lambda_rank.loc[:,column].shift(lag) for lag in range(round(lag_ws+1))}\n",
    "        lagged = pd.concat(lagged_cols, axis=1).dropna().resample('M').last()\n",
    "        lambda_rank_L = pd.concat([lambda_rank_L, lagged], axis=1)\n",
    "\n",
    "    MI_filter = MI.loc[MI.index.to_series().apply(lambda x: pd.Period(x, freq='M')).isin(lambda_rank_L.index.to_period('M').unique())]\n",
    "\n",
    "    #####\n",
    "\n",
    "    X = lambda_rank_L.copy()\n",
    "    y = MI_filter['Recession']\n",
    "\n",
    "    for col in X.columns:\n",
    "        X[col] = pd.to_numeric(X[col], errors='coerce')\n",
    "\n",
    "    cut = 5\n",
    "    n = len(X)\n",
    "\n",
    "    models = {\n",
    "        \"LR\": LogisticRegression(max_iter=10000, random_state=42, multi_class='ovr'),   # Logistic Regression\n",
    "        \"XGB\": XGBClassifier(eval_metric='mlogloss', random_state=42)\n",
    "    }\n",
    "\n",
    "    results = pd.DataFrame(columns=[\"LR_AUC\", \"LR_Brier\", \"LR_AUC\", \"LR_Brier\", \"XGB_AUC\", \"XGB_Brier\", \"XGB_AUC\", \"XGB_Brier\"])\n",
    "    all_predictions = {model_name: pd.Series([None] * n, index=MI_filter.index) for model_name in models.keys()}\n",
    "\n",
    "    n_samples = X.shape[0]\n",
    "    split_size = n_samples // cut\n",
    "\n",
    "    for i in tqdm(range(cut)):\n",
    "        start_test = i * split_size\n",
    "        end_test = (i + 1) * split_size if i < cut - 1 else n_samples\n",
    "\n",
    "        X_test = X[start_test:end_test]\n",
    "        y_test = y[start_test:end_test]\n",
    "\n",
    "        indices = list(range(0, start_test)) + list(range(end_test, n_samples))\n",
    "        X_train = X.iloc[indices]\n",
    "        y_train = y.iloc[indices]\n",
    "\n",
    "        row_results = []\n",
    "\n",
    "        for name, model in models.items():\n",
    "            model.fit(X_train, y_train)\n",
    "            \n",
    "            y_pred_train = model.predict(X_train)\n",
    "            y_pred_test = model.predict(X_test)\n",
    "            y_pred_proba_train = model.predict_proba(X_train)[:, 1]\n",
    "            y_pred_proba_test = model.predict_proba(X_test)[:, 1]\n",
    "\n",
    "            auc_score_train = roc_auc_score(y_train, y_pred_proba_train)\n",
    "            brier_score_train = brier_score_loss(y_train, y_pred_proba_train)\n",
    "            auc_score_test = roc_auc_score(y_test, y_pred_proba_test)\n",
    "            brier_score_test = brier_score_loss(y_test, y_pred_proba_test)\n",
    "\n",
    "            row_results.extend([round(auc_score_train, 4), round(brier_score_train, 4), round(auc_score_test, 4), round(brier_score_test, 4)])\n",
    "\n",
    "            all_predictions[name].iloc[start_test:end_test] = y_pred_proba_test\n",
    "\n",
    "            fig, ax = plt.subplots(figsize=(20, 4))\n",
    "            color = 'darkgoldenrod' if name == 'Logistic Regression' else 'darkblue'\n",
    "            ax.scatter(MI_filter.index[indices], y_train, label='Actual', color='darkred', marker='s', s=14)\n",
    "            ax.plot(MI_filter.index[indices], y_pred_proba_train, label=f'Predicted by {name}', linewidth=4, color=color, alpha=0.2)\n",
    "            count = 0\n",
    "            for idx in indices:\n",
    "                if y_train.loc[MI_filter.index[idx]] == 1 and y_pred_proba_train[count] < 0.5:\n",
    "                    ax.scatter(MI_filter.index[idx], y_pred_proba_train[count], color='darkgreen', marker='x', s=50, lw=2.5, alpha=1)\n",
    "                elif y_train.loc[MI_filter.index[idx]] == 0 and y_pred_proba_train[count] > 0.5:\n",
    "                    ax.scatter(MI_filter.index[idx], y_pred_proba_train[count], color='darkgreen', marker='^', s=40, lw=1, alpha=1)\n",
    "                else:\n",
    "                    ax.scatter(MI_filter.index[idx], y_pred_proba_train[count], color=color, alpha=1)\n",
    "                count += 1\n",
    "            ax.axhline(y=0.5, color='k', linestyle='--', alpha=0.4)\n",
    "            ax.set_xlim(y.index[0], y.index[-1])\n",
    "            ax.set_ylim(-0.1, 1.1)\n",
    "            ax.tick_params(axis='y', labelsize=8, width=2)\n",
    "            ax.tick_params(axis='x', labelsize=8, width=2)\n",
    "            ax.xaxis.set_major_locator(mdates.YearLocator())\n",
    "            ax.xaxis.set_major_formatter(mdates.DateFormatter('%Y'))\n",
    "            plt.tight_layout()\n",
    "            plt.savefig(f'{output_path}/IN {feature} {name} F{i+1}.png', dpi=144, transparent=True)\n",
    "            plt.close()\n",
    "\n",
    "        results.loc[i] = row_results\n",
    "\n",
    "    results.columns = [\"LR_AUC\", \"LR_Brier\", \"LR_AUC\", \"LR_Brier\", \"XGB_AUC\", \"XGB_Brier\", \"XGB_AUC\", \"XGB_Brier\"]\n",
    "\n",
    "    average_results = results.mean().to_frame().T\n",
    "    average_results.index = [\"-\"]\n",
    "\n",
    "    print(results.round(4))\n",
    "    print(average_results.round(4))\n",
    "    print()\n",
    "\n",
    "    for name in models.keys():\n",
    "        plt.figure(figsize=(20, 4))\n",
    "        color = 'darkgoldenrod' if name == 'Logistic Regression' else 'darkblue'\n",
    "        plt.scatter(MI_filter.index, y, label='Actual', color='darkred', marker='s', s=14)\n",
    "        # plt.scatter(MI_filter.index, all_predictions[name], label=f'Predicted by {name}', s=20)\n",
    "        plt.plot(MI_filter.index, all_predictions[name], label=f'Predicted by {name}', linewidth=4, color=color, alpha=0.2)\n",
    "        for i in range(len(all_predictions[name])):\n",
    "            if y.iloc[i] == 1 and all_predictions[name].values[i] < 0.5:\n",
    "                plt.scatter(all_predictions[name].index[i], all_predictions[name].values[i], color='darkgreen', marker='x', s=50, lw=2.5, alpha=1)\n",
    "            elif y.iloc[i] == 0 and all_predictions[name].values[i] > 0.5:\n",
    "                plt.scatter(all_predictions[name].index[i], all_predictions[name].values[i], color='darkgreen', marker='^', s=40, lw=1, alpha=1)\n",
    "            else:\n",
    "                plt.scatter(all_predictions[name].index[i], all_predictions[name].values[i], color=color, alpha=1)\n",
    "\n",
    "        plt.axhline(y=0.5, color='k', linestyle='--', alpha=0.4)\n",
    "        plt.ylim(-0.1, 1.1)\n",
    "        plt.yticks(fontsize=14, fontweight='heavy')\n",
    "        plt.xticks(fontsize=14, fontweight='heavy')\n",
    "        plt.gca().xaxis.set_major_locator(mdates.YearLocator())\n",
    "        plt.gca().xaxis.set_major_formatter(mdates.DateFormatter('%Y'))\n",
    "        plt.tight_layout()\n",
    "        plt.savefig(f'{output_path}/OUT {feature} {name}.png', dpi=144, transparent=True)\n",
    "        plt.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lambdas, with Feature Selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "companies = ['Cap_63', 'Cap_126', 'Ele_63', 'Ele_126', 'Fin_63', 'Fin_126', 'FinEle_63', 'FinEle_126']\n",
    "company = 'Cap_63'\n",
    "for company in companies:\n",
    "    sector, ws = company.split('_')[0], int(company.split('_')[1])\n",
    "\n",
    "    print(company)\n",
    "    top = 20\n",
    "    lag_ws = 63\n",
    "    input_path1 = f'../03 Modeling/{company}'\n",
    "    input_path2 = f'../01 Raw Data/{sector}'\n",
    "    output_path = f'{company}'\n",
    "    if not os.path.exists(output_path):\n",
    "        os.makedirs(output_path)\n",
    "\n",
    "    full_lambda = pd.read_csv(f'{input_path1}/full_lambda.csv', index_col='Date', parse_dates=['Date'])\n",
    "    stock_rank = pd.read_csv(f'{input_path2}/Top rank company.csv', index_col='Date', parse_dates=['Date'])\n",
    "    stock_rank = stock_rank.astype(str)\n",
    "\n",
    "    lambda_rank = pd.DataFrame(index=full_lambda.index, columns=range(1, 21))\n",
    "    for date in full_lambda.index:\n",
    "        for col in lambda_rank.columns:\n",
    "            company_id = str(stock_rank.loc[date, str(col)]).split('.')[0]\n",
    "            lambda_rank.loc[date, col] = full_lambda.loc[date, company_id]\n",
    "    scaler = MinMaxScaler()\n",
    "    lambda_rank[:] = scaler.fit_transform(lambda_rank.values)\n",
    "    lambda_rank_L = pd.DataFrame()\n",
    "    for column in lambda_rank:\n",
    "        lagged_cols = {f'{column}_{lag}': lambda_rank.loc[:,column].shift(lag) for lag in range(round(lag_ws+1))}\n",
    "        lagged = pd.concat(lagged_cols, axis=1).dropna().resample('M').last()\n",
    "        lambda_rank_L = pd.concat([lambda_rank_L, lagged], axis=1)\n",
    "\n",
    "    MI_filter = MI.loc[MI.index.to_series().apply(lambda x: pd.Period(x, freq='M')).isin(lambda_rank_L.index.to_period('M').unique())]\n",
    "\n",
    "    #####\n",
    "\n",
    "    X = lambda_rank_L.copy()\n",
    "    y = MI_filter['Recession']\n",
    "\n",
    "    for col in X.columns:\n",
    "        X[col] = pd.to_numeric(X[col], errors='coerce')\n",
    "\n",
    "    cut = 5\n",
    "    n = len(X)\n",
    "\n",
    "    models = {\n",
    "        \"Logistic Regression\": LogisticRegression(max_iter=10000, random_state=42, multi_class='ovr'),\n",
    "        \"XGBoost\": XGBClassifier(eval_metric='mlogloss', random_state=42)\n",
    "    }\n",
    "\n",
    "    results = pd.DataFrame(columns=[\"LR_AUC\", \"LR_Brier\", \"LR_AUC\", \"LR_Brier\", \"XGB_AUC\", \"XGB_Brier\", \"XGB_AUC\", \"XGB_Brier\"])\n",
    "    all_predictions = {model_name: pd.Series([None] * n, index=MI_filter.index) for model_name in models.keys()}\n",
    "\n",
    "    n_samples = X.shape[0]\n",
    "    split_size = n_samples // cut\n",
    "\n",
    "    for i in tqdm(range(cut)):\n",
    "        start_test = i * split_size\n",
    "        end_test = (i + 1) * split_size if i < cut - 1 else n_samples\n",
    "\n",
    "        X_test = X[start_test:end_test]\n",
    "        y_test = y[start_test:end_test]\n",
    "\n",
    "        indices = list(range(0, start_test)) + list(range(end_test, n_samples))\n",
    "        X_train = X.iloc[indices]\n",
    "        y_train = y.iloc[indices]\n",
    "\n",
    "        lasso_cv = LassoCV(cv=10, random_state=42, max_iter=10000, n_jobs=-1).fit(X_train, y_train)\n",
    "        best_alpha = lasso_cv.alpha_\n",
    "        lasso = Lasso(alpha=best_alpha)\n",
    "        lasso.fit(X_train, y_train)\n",
    "\n",
    "        selected_features = np.where(lasso.coef_ != 0)[0]\n",
    "\n",
    "        count = 0\n",
    "        while len(selected_features) < top:\n",
    "            count += 1\n",
    "            lasso = Lasso(alpha=best_alpha*(0.5**count))\n",
    "            lasso.fit(X_train, y_train)\n",
    "            selected_features = np.where(lasso.coef_ != 0)[0]\n",
    "\n",
    "        X_train = X_train.iloc[:, selected_features]\n",
    "        X_test = X_test.iloc[:, selected_features]\n",
    "\n",
    "        row_results = []\n",
    "\n",
    "        for name, model in models.items():\n",
    "            model.fit(X_train, y_train)\n",
    "            \n",
    "            y_pred_train = model.predict(X_train)\n",
    "            y_pred_test = model.predict(X_test)\n",
    "            y_pred_proba_train = model.predict_proba(X_train)[:, 1]\n",
    "            y_pred_proba_test = model.predict_proba(X_test)[:, 1]\n",
    "\n",
    "            auc_score_train = roc_auc_score(y_train, y_pred_proba_train)\n",
    "            brier_score_train = brier_score_loss(y_train, y_pred_proba_train)\n",
    "            auc_score_test = roc_auc_score(y_test, y_pred_proba_test)\n",
    "            brier_score_test = brier_score_loss(y_test, y_pred_proba_test)\n",
    "\n",
    "            row_results.extend([round(auc_score_train, 4), round(brier_score_train, 4), round(auc_score_test, 4), round(brier_score_test, 4)])\n",
    "\n",
    "            all_predictions[name].iloc[start_test:end_test] = y_pred_proba_test\n",
    "\n",
    "        results.loc[i] = row_results\n",
    "\n",
    "    results.columns = [\"LR_AUC\", \"LR_Brier\", \"LR_AUC\", \"LR_Brier\", \"XGB_AUC\", \"XGB_Brier\", \"XGB_AUC\", \"XGB_Brier\"]\n",
    "\n",
    "    average_results = results.mean().to_frame().T\n",
    "    average_results.index = [\"-\"]\n",
    "\n",
    "    print(results.round(4))\n",
    "    print(average_results.round(4))\n",
    "    print()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Benchmark: VIXTWN without Feature Selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lag_ws = 189\n",
    "\n",
    "VIX_L = pd.DataFrame()\n",
    "lagged_cols = {f'VIX_lag_{lag}': VIX['VIXTWN'].shift(lag) for lag in range(round(lag_ws+1))}\n",
    "VIX_L = pd.concat(lagged_cols, axis=1).dropna().resample('M').last()\n",
    "VIX_L = VIX_L.dropna()\n",
    "VIX_L = VIX_L.resample('M').last()\n",
    "\n",
    "VIX_L = VIX_L.loc[VIX_L.index.intersection(lambda_rank_L.index)]\n",
    "\n",
    "#####\n",
    "\n",
    "X = VIX_L.copy()\n",
    "y = MI_filter['Recession']\n",
    "\n",
    "for col in X.columns:\n",
    "    X[col] = pd.to_numeric(X[col], errors='coerce')\n",
    "\n",
    "cut = 5\n",
    "n = len(X)\n",
    "\n",
    "models = {\n",
    "    \"Logistic Regression\": LogisticRegression(max_iter=10000, random_state=42, multi_class='ovr'),\n",
    "    \"XGBoost\": XGBClassifier(eval_metric='mlogloss', random_state=42)\n",
    "}\n",
    "\n",
    "results = pd.DataFrame(columns=[\"LR_AUC\", \"LR_Brier\", \"LR_AUC\", \"LR_Brier\", \"XGB_AUC\", \"XGB_Brier\", \"XGB_AUC\", \"XGB_Brier\"])\n",
    "all_predictions = {model_name: pd.Series([None] * n, index=MI_filter.index) for model_name in models.keys()}\n",
    "\n",
    "n_samples = X.shape[0]\n",
    "split_size = n_samples // cut\n",
    "\n",
    "for i in tqdm(range(cut)):\n",
    "    start_test = i * split_size\n",
    "    end_test = (i + 1) * split_size if i < cut - 1 else n_samples\n",
    "\n",
    "    X_test = X[start_test:end_test]\n",
    "    y_test = y[start_test:end_test]\n",
    "\n",
    "    indices = list(range(0, start_test)) + list(range(end_test, n_samples))\n",
    "    X_train = X.iloc[indices]\n",
    "    y_train = y.iloc[indices]\n",
    "\n",
    "    row_results = []\n",
    "\n",
    "    for name, model in models.items():\n",
    "        model.fit(X_train, y_train)\n",
    "        \n",
    "        y_pred_train = model.predict(X_train)\n",
    "        y_pred_test = model.predict(X_test)\n",
    "        y_pred_proba_train = model.predict_proba(X_train)[:, 1]\n",
    "        y_pred_proba_test = model.predict_proba(X_test)[:, 1]\n",
    "\n",
    "        auc_score_train = roc_auc_score(y_train, y_pred_proba_train)\n",
    "        brier_score_train = brier_score_loss(y_train, y_pred_proba_train)\n",
    "        auc_score_test = roc_auc_score(y_test, y_pred_proba_test)\n",
    "        brier_score_test = brier_score_loss(y_test, y_pred_proba_test)\n",
    "\n",
    "        row_results.extend([round(auc_score_train, 4), round(brier_score_train, 4), round(auc_score_test, 4), round(brier_score_test, 4)])\n",
    "\n",
    "        all_predictions[name].iloc[start_test:end_test] = y_pred_proba_test\n",
    "\n",
    "    results.loc[i] = row_results\n",
    "\n",
    "results.columns = [\"LR_AUC\", \"LR_Brier\", \"LR_AUC\", \"LR_Brier\", \"XGB_AUC\", \"XGB_Brier\", \"XGB_AUC\", \"XGB_Brier\"]\n",
    "\n",
    "average_results = results.mean().to_frame().T\n",
    "average_results.index = [\"-\"]\n",
    "\n",
    "print(results.round(4))\n",
    "print(average_results.round(4))\n",
    "print()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Benchmark: VIXTWN with Feature Selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lag_ws = 189\n",
    "\n",
    "VIX_L = pd.DataFrame()\n",
    "lagged_cols = {f'VIX_lag_{lag}': VIX['VIXTWN'].shift(lag) for lag in range(round(lag_ws+1))}\n",
    "VIX_L = pd.concat(lagged_cols, axis=1).dropna().resample('M').last()\n",
    "VIX_L = VIX_L.dropna()\n",
    "VIX_L = VIX_L.resample('M').last()\n",
    "\n",
    "VIX_L = VIX_L.loc[VIX_L.index.intersection(lambda_rank_L.index)]\n",
    "\n",
    "#####\n",
    "\n",
    "X = VIX_L.copy()\n",
    "y = MI_filter['Recession']\n",
    "\n",
    "for col in X.columns:\n",
    "    X[col] = pd.to_numeric(X[col], errors='coerce')\n",
    "\n",
    "cut = 5\n",
    "n = len(X)\n",
    "\n",
    "models = {\n",
    "    \"Logistic Regression\": LogisticRegression(max_iter=10000, random_state=42, multi_class='ovr'),\n",
    "    \"XGBoost\": XGBClassifier(eval_metric='mlogloss', random_state=42)\n",
    "}\n",
    "\n",
    "results = pd.DataFrame(columns=[\"LR_AUC\", \"LR_Brier\", \"LR_AUC\", \"LR_Brier\", \"XGB_AUC\", \"XGB_Brier\", \"XGB_AUC\", \"XGB_Brier\"])\n",
    "all_predictions = {model_name: pd.Series([None] * n, index=MI_filter.index) for model_name in models.keys()}\n",
    "\n",
    "n_samples = X.shape[0]\n",
    "split_size = n_samples // cut\n",
    "\n",
    "for i in tqdm(range(cut)):\n",
    "    start_test = i * split_size\n",
    "    end_test = (i + 1) * split_size if i < cut - 1 else n_samples\n",
    "\n",
    "    X_test = X[start_test:end_test]\n",
    "    y_test = y[start_test:end_test]\n",
    "\n",
    "    indices = list(range(0, start_test)) + list(range(end_test, n_samples))\n",
    "    X_train = X.iloc[indices]\n",
    "    y_train = y.iloc[indices]\n",
    "\n",
    "    lasso_cv = LassoCV(cv=5, random_state=42, max_iter=100000, n_jobs=-1).fit(X_train, y_train)\n",
    "    best_alpha = lasso_cv.alpha_\n",
    "    lasso = Lasso(alpha=best_alpha)\n",
    "    lasso.fit(X_train, y_train)\n",
    "\n",
    "    selected_features = np.where(lasso.coef_ != 0)[0]\n",
    "\n",
    "    count = 0\n",
    "    while len(selected_features) < top:\n",
    "        count += 1\n",
    "        lasso = Lasso(alpha=best_alpha*(0.5**count))\n",
    "        lasso.fit(X_train, y_train)\n",
    "        selected_features = np.where(lasso.coef_ != 0)[0]\n",
    "\n",
    "    X_train = X_train.iloc[:, selected_features]\n",
    "    X_test = X_test.iloc[:, selected_features]\n",
    "\n",
    "    row_results = []\n",
    "\n",
    "    for name, model in models.items():\n",
    "        model.fit(X_train, y_train)\n",
    "        \n",
    "        y_pred_train = model.predict(X_train)\n",
    "        y_pred_test = model.predict(X_test)\n",
    "        y_pred_proba_train = model.predict_proba(X_train)[:, 1]\n",
    "        y_pred_proba_test = model.predict_proba(X_test)[:, 1]\n",
    "\n",
    "        auc_score_train = roc_auc_score(y_train, y_pred_proba_train)\n",
    "        brier_score_train = brier_score_loss(y_train, y_pred_proba_train)\n",
    "        auc_score_test = roc_auc_score(y_test, y_pred_proba_test)\n",
    "        brier_score_test = brier_score_loss(y_test, y_pred_proba_test)\n",
    "\n",
    "        row_results.extend([round(auc_score_train, 4), round(brier_score_train, 4), round(auc_score_test, 4), round(brier_score_test, 4)])\n",
    "\n",
    "        all_predictions[name].iloc[start_test:end_test] = y_pred_proba_test\n",
    "\n",
    "    results.loc[i] = row_results\n",
    "\n",
    "results.columns = [\"LR_AUC\", \"LR_Brier\", \"LR_AUC\", \"LR_Brier\", \"XGB_AUC\", \"XGB_Brier\", \"XGB_AUC\", \"XGB_Brier\"]\n",
    "\n",
    "average_results = results.mean().to_frame().T\n",
    "average_results.index = [\"-\"]\n",
    "\n",
    "print(results.round(4))\n",
    "print(average_results.round(4))\n",
    "print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# FRM, without Feature selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "companies = ['Cap_63', 'Cap_126', 'Ele_63', 'Ele_126', 'Fin_63', 'Fin_126', 'FinEle_63', 'FinEle_126']\n",
    "company = 'Cap_63'\n",
    "for company in companies:\n",
    "    sector, ws = company.split('_')[0], int(company.split('_')[1])\n",
    "\n",
    "    print(company)\n",
    "    top = 20\n",
    "    lag_ws = 63\n",
    "    input_path1 = f'../03 Modeling/{company}'\n",
    "    input_path2 = f'../01 Raw Data/{sector}'\n",
    "    output_path = f'{company}'\n",
    "    if not os.path.exists(output_path):\n",
    "        os.makedirs(output_path)\n",
    "\n",
    "    full_lambda = pd.read_csv(f'{input_path1}/full_lambda.csv', index_col='Date', parse_dates=['Date'])\n",
    "    FRM = full_lambda.mean(axis=1) * full_lambda.shape[1] / top\n",
    "\n",
    "    FRM_L = pd.DataFrame()\n",
    "    lagged_cols = {f'FRM_lag_{lag}': FRM.shift(lag) for lag in range(round(lag_ws+1))}\n",
    "    FRM_L = pd.concat(lagged_cols, axis=1).dropna().resample('M').last()\n",
    "    FRM_L = FRM_L.dropna()\n",
    "    FRM_L = FRM_L.resample('M').last()\n",
    "\n",
    "    MI_filter = MI.loc[MI.index.to_series().apply(lambda x: pd.Period(x, freq='M')).isin(FRM_L.index.to_period('M').unique())]\n",
    "\n",
    "    #####\n",
    "\n",
    "    X = FRM_L.copy()\n",
    "    y = MI_filter['Recession']\n",
    "\n",
    "    for col in X.columns:\n",
    "        X[col] = pd.to_numeric(X[col], errors='coerce')\n",
    "\n",
    "    cut = 5\n",
    "    n = len(X)\n",
    "\n",
    "    models = {\n",
    "        \"Logistic Regression\": LogisticRegression(max_iter=10000, random_state=42, multi_class='ovr'),\n",
    "        \"XGBoost\": XGBClassifier(eval_metric='mlogloss', random_state=42)\n",
    "    }\n",
    "\n",
    "    results = pd.DataFrame(columns=[\"LR_AUC\", \"LR_Brier\", \"LR_AUC\", \"LR_Brier\", \"XGB_AUC\", \"XGB_Brier\", \"XGB_AUC\", \"XGB_Brier\"])\n",
    "    all_predictions = {model_name: pd.Series([None] * n, index=MI_filter.index) for model_name in models.keys()}\n",
    "\n",
    "    n_samples = X.shape[0]\n",
    "    split_size = n_samples // cut\n",
    "\n",
    "    for i in tqdm(range(cut)):\n",
    "        start_test = i * split_size\n",
    "        end_test = (i + 1) * split_size if i < cut - 1 else n_samples\n",
    "\n",
    "        X_test = X[start_test:end_test]\n",
    "        y_test = y[start_test:end_test]\n",
    "\n",
    "        indices = list(range(0, start_test)) + list(range(end_test, n_samples))\n",
    "        X_train = X.iloc[indices]\n",
    "        y_train = y.iloc[indices]\n",
    "\n",
    "        row_results = []\n",
    "\n",
    "        for name, model in models.items():\n",
    "            model.fit(X_train, y_train)\n",
    "            \n",
    "            y_pred_train = model.predict(X_train)\n",
    "            y_pred_test = model.predict(X_test)\n",
    "            y_pred_proba_train = model.predict_proba(X_train)[:, 1]\n",
    "            y_pred_proba_test = model.predict_proba(X_test)[:, 1]\n",
    "\n",
    "            auc_score_train = roc_auc_score(y_train, y_pred_proba_train)\n",
    "            brier_score_train = brier_score_loss(y_train, y_pred_proba_train)\n",
    "            auc_score_test = roc_auc_score(y_test, y_pred_proba_test)\n",
    "            brier_score_test = brier_score_loss(y_test, y_pred_proba_test)\n",
    "\n",
    "            row_results.extend([round(auc_score_train, 4), round(brier_score_train, 4), round(auc_score_test, 4), round(brier_score_test, 4)])\n",
    "\n",
    "            all_predictions[name].iloc[start_test:end_test] = y_pred_proba_test\n",
    "\n",
    "            fig, ax = plt.subplots(figsize=(20, 4))\n",
    "            color = 'darkgoldenrod' if name == 'Logistic Regression' else 'darkblue'\n",
    "            ax.scatter(MI_filter.index[indices], y_train, label='Actual', color='darkred', marker='s', s=14)\n",
    "            ax.plot(MI_filter.index[indices], y_pred_proba_train, label=f'Predicted by {name}', linewidth=4, color=color, alpha=0.2)\n",
    "            count = 0\n",
    "            for idx in indices:\n",
    "                if y_train.loc[MI_filter.index[idx]] == 1 and y_pred_proba_train[count] < 0.5:\n",
    "                    ax.scatter(MI_filter.index[idx], y_pred_proba_train[count], color='darkgreen', marker='x', s=50, lw=2.5, alpha=1)\n",
    "                elif y_train.loc[MI_filter.index[idx]] == 0 and y_pred_proba_train[count] > 0.5:\n",
    "                    ax.scatter(MI_filter.index[idx], y_pred_proba_train[count], color='darkgreen', marker='^', s=40, lw=1, alpha=1)\n",
    "                else:\n",
    "                    ax.scatter(MI_filter.index[idx], y_pred_proba_train[count], color=color, alpha=1)\n",
    "                count += 1\n",
    "            ax.axhline(y=0.5, color='k', linestyle='--', alpha=0.4)\n",
    "            ax.set_xlim(y.index[0], y.index[-1])\n",
    "            ax.set_ylim(-0.1, 1.1)\n",
    "            ax.tick_params(axis='y', labelsize=8, width=2)\n",
    "            ax.tick_params(axis='x', labelsize=8, width=2)\n",
    "            ax.xaxis.set_major_locator(mdates.YearLocator())\n",
    "            ax.xaxis.set_major_formatter(mdates.DateFormatter('%Y'))\n",
    "            plt.tight_layout()\n",
    "            plt.savefig(f'{output_path}/IN FRM {feature} {name} F{i+1}.png', dpi=144, transparent=True)\n",
    "            plt.close()\n",
    "\n",
    "        results.loc[i] = row_results\n",
    "\n",
    "    results.columns = [\"LR_AUC\", \"LR_Brier\", \"LR_AUC\", \"LR_Brier\", \"XGB_AUC\", \"XGB_Brier\", \"XGB_AUC\", \"XGB_Brier\"]\n",
    "\n",
    "    average_results = results.mean().to_frame().T\n",
    "    average_results.index = [\"-\"]\n",
    "\n",
    "    print(results.round(4))\n",
    "    print(average_results.round(4))\n",
    "    print()\n",
    "\n",
    "    for name in models.keys():\n",
    "        plt.figure(figsize=(20, 4))\n",
    "        color = 'darkgoldenrod' if name == 'Logistic Regression' else 'darkblue'\n",
    "        plt.scatter(MI_filter.index, y, label='Actual', color='darkred', marker='s', s=14)\n",
    "        # plt.scatter(MI_filter.index, all_predictions[name], label=f'Predicted by {name}', s=20)\n",
    "        plt.plot(MI_filter.index, all_predictions[name], label=f'Predicted by {name}', linewidth=4, color=color, alpha=0.2)\n",
    "        for i in range(len(all_predictions[name])):\n",
    "            if y.iloc[i] == 1 and all_predictions[name].values[i] < 0.5:\n",
    "                plt.scatter(all_predictions[name].index[i], all_predictions[name].values[i], color='darkgreen', marker='x', s=50, lw=2.5, alpha=1)\n",
    "            elif y.iloc[i] == 0 and all_predictions[name].values[i] > 0.5:\n",
    "                plt.scatter(all_predictions[name].index[i], all_predictions[name].values[i], color='darkgreen', marker='^', s=40, lw=1, alpha=1)\n",
    "            else:\n",
    "                plt.scatter(all_predictions[name].index[i], all_predictions[name].values[i], color=color, alpha=1)\n",
    "\n",
    "        plt.axhline(y=0.5, color='k', linestyle='--', alpha=0.4)\n",
    "        plt.ylim(-0.1, 1.1)\n",
    "        plt.yticks(fontsize=14, fontweight='heavy')\n",
    "        plt.xticks(fontsize=14, fontweight='heavy')\n",
    "        plt.gca().xaxis.set_major_locator(mdates.YearLocator())\n",
    "        plt.gca().xaxis.set_major_formatter(mdates.DateFormatter('%Y'))\n",
    "        plt.tight_layout()\n",
    "        plt.savefig(f'{output_path}/OUT FRM {feature} {name}.png', dpi=144, transparent=True)\n",
    "        plt.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# FRM, with Feature selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "companies = ['Cap_63', 'Cap_126', 'Ele_63', 'Ele_126', 'Fin_63', 'Fin_126', 'FinEle_63', 'FinEle_126']\n",
    "company = 'Cap_63'\n",
    "for company in companies:\n",
    "    sector, ws = company.split('_')[0], int(company.split('_')[1])\n",
    "\n",
    "    print(company)\n",
    "    top = 20\n",
    "    lag_ws = 63\n",
    "    input_path1 = f'../03 Modeling/{company}'\n",
    "    input_path2 = f'../01 Raw Data/{sector}'\n",
    "    output_path = f'{company}'\n",
    "    if not os.path.exists(output_path):\n",
    "        os.makedirs(output_path)\n",
    "\n",
    "    full_lambda = pd.read_csv(f'{input_path1}/full_lambda.csv', index_col='Date', parse_dates=['Date'])\n",
    "    FRM = full_lambda.mean(axis=1) * full_lambda.shape[1] / top\n",
    "\n",
    "    FRM_L = pd.DataFrame()\n",
    "    lagged_cols = {f'FRM_lag_{lag}': FRM.shift(lag) for lag in range(round(lag_ws+1))}\n",
    "    FRM_L = pd.concat(lagged_cols, axis=1).dropna().resample('M').last()\n",
    "    FRM_L = FRM_L.dropna()\n",
    "    FRM_L = FRM_L.resample('M').last()\n",
    "\n",
    "    MI_filter = MI.loc[MI.index.to_series().apply(lambda x: pd.Period(x, freq='M')).isin(FRM_L.index.to_period('M').unique())]\n",
    "\n",
    "    #####\n",
    "\n",
    "    X = FRM_L.copy()\n",
    "    y = MI_filter['Recession']\n",
    "\n",
    "    for col in X.columns:\n",
    "        X[col] = pd.to_numeric(X[col], errors='coerce')\n",
    "\n",
    "    cut = 5\n",
    "    n = len(X)\n",
    "\n",
    "    models = {\n",
    "        \"Logistic Regression\": LogisticRegression(max_iter=10000, random_state=42, multi_class='ovr'),\n",
    "        \"XGBoost\": XGBClassifier(eval_metric='mlogloss', random_state=42)\n",
    "    }\n",
    "\n",
    "    results = pd.DataFrame(columns=[\"LR_AUC\", \"LR_Brier\", \"LR_AUC\", \"LR_Brier\", \"XGB_AUC\", \"XGB_Brier\", \"XGB_AUC\", \"XGB_Brier\"])\n",
    "    all_predictions = {model_name: pd.Series([None] * n, index=MI_filter.index) for model_name in models.keys()}\n",
    "\n",
    "    n_samples = X.shape[0]\n",
    "    split_size = n_samples // cut\n",
    "\n",
    "    for i in tqdm(range(cut)):\n",
    "        start_test = i * split_size\n",
    "        end_test = (i + 1) * split_size if i < cut - 1 else n_samples\n",
    "\n",
    "        X_test = X[start_test:end_test]\n",
    "        y_test = y[start_test:end_test]\n",
    "\n",
    "        indices = list(range(0, start_test)) + list(range(end_test, n_samples))\n",
    "        X_train = X.iloc[indices]\n",
    "        y_train = y.iloc[indices]\n",
    "\n",
    "        lasso_cv = LassoCV(cv=10, random_state=42, max_iter=10000, n_jobs=-1).fit(X_train, y_train)\n",
    "        best_alpha = lasso_cv.alpha_\n",
    "        lasso = Lasso(alpha=best_alpha)\n",
    "        lasso.fit(X_train, y_train)\n",
    "\n",
    "        selected_features = np.where(lasso.coef_ != 0)[0]\n",
    "\n",
    "        count = 0\n",
    "        while len(selected_features) < top:\n",
    "            count += 1\n",
    "            lasso = Lasso(alpha=best_alpha*(0.5**count))\n",
    "            lasso.fit(X_train, y_train)\n",
    "            selected_features = np.where(lasso.coef_ != 0)[0]\n",
    "\n",
    "        X_train = X_train.iloc[:, selected_features]\n",
    "        X_test = X_test.iloc[:, selected_features]\n",
    "\n",
    "        row_results = []\n",
    "\n",
    "        for name, model in models.items():\n",
    "            model.fit(X_train, y_train)\n",
    "            \n",
    "            y_pred_train = model.predict(X_train)\n",
    "            y_pred_test = model.predict(X_test)\n",
    "            y_pred_proba_train = model.predict_proba(X_train)[:, 1]\n",
    "            y_pred_proba_test = model.predict_proba(X_test)[:, 1]\n",
    "\n",
    "            auc_score_train = roc_auc_score(y_train, y_pred_proba_train)\n",
    "            brier_score_train = brier_score_loss(y_train, y_pred_proba_train)\n",
    "            auc_score_test = roc_auc_score(y_test, y_pred_proba_test)\n",
    "            brier_score_test = brier_score_loss(y_test, y_pred_proba_test)\n",
    "\n",
    "            row_results.extend([round(auc_score_train, 4), round(brier_score_train, 4), round(auc_score_test, 4), round(brier_score_test, 4)])\n",
    "\n",
    "            all_predictions[name].iloc[start_test:end_test] = y_pred_proba_test\n",
    "\n",
    "        results.loc[i] = row_results\n",
    "\n",
    "    results.columns = [\"LR_AUC\", \"LR_Brier\", \"LR_AUC\", \"LR_Brier\", \"XGB_AUC\", \"XGB_Brier\", \"XGB_AUC\", \"XGB_Brier\"]\n",
    "\n",
    "    average_results = results.mean().to_frame().T\n",
    "    average_results.index = [\"-\"]\n",
    "\n",
    "    print(results.round(4))\n",
    "    print(average_results.round(4))\n",
    "    print()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "companies = ['Cap_63', 'Cap_126', 'Ele_63', 'Ele_126', 'Fin_63', 'Fin_126', 'FinEle_63', 'FinEle_126']\n",
    "\n",
    "for company in companies:\n",
    "    sector, ws = company.split('_')[0], int(company.split('_')[1])\n",
    "\n",
    "    print(company)\n",
    "    top = 20\n",
    "    lag_ws = 63\n",
    "    input_path1 = f'../03 Modeling/{company}'\n",
    "    input_path2 = f'../01 Raw Data/{sector}'\n",
    "    output_path = f'{company}'\n",
    "    if not os.path.exists(output_path):\n",
    "        os.makedirs(output_path)\n",
    "\n",
    "    full_lambda = pd.read_csv(f'{input_path1}/full_lambda.csv', index_col='Date', parse_dates=['Date'])\n",
    "    stock_rank = pd.read_csv(f'{input_path2}/Top rank company.csv', index_col='Date', parse_dates=['Date'])\n",
    "    stock_rank = stock_rank.astype(str)\n",
    "\n",
    "    lambda_rank = pd.DataFrame(index=full_lambda.index, columns=range(1, 21))\n",
    "    for date in full_lambda.index:\n",
    "        for col in lambda_rank.columns:\n",
    "            company_id = str(stock_rank.loc[date, str(col)]).split('.')[0]\n",
    "            lambda_rank.loc[date, col] = full_lambda.loc[date, company_id]\n",
    "    scaler = MinMaxScaler()\n",
    "    lambda_rank[:] = scaler.fit_transform(lambda_rank.values)\n",
    "    lambda_rank_L = pd.DataFrame()\n",
    "    for column in lambda_rank:\n",
    "        lagged_cols = {f'{column}_{lag}': lambda_rank.loc[:,column].shift(lag) for lag in range(round(lag_ws+1))}\n",
    "        lagged = pd.concat(lagged_cols, axis=1).dropna().resample('M').last()\n",
    "        lambda_rank_L = pd.concat([lambda_rank_L, lagged], axis=1)\n",
    "\n",
    "    MI_filter = MI.loc[MI.index.to_series().apply(lambda x: pd.Period(x, freq='M')).isin(lambda_rank_L.index.to_period('M').unique())]\n",
    "\n",
    "    X = lambda_rank_L.copy()\n",
    "    y = MI_filter['Recession']\n",
    "\n",
    "    for col in X.columns:\n",
    "        X[col] = pd.to_numeric(X[col], errors='coerce')\n",
    "\n",
    "    cut = 5\n",
    "    n = len(X)\n",
    "\n",
    "    selected_features_list = []\n",
    "    total_features_list = []\n",
    "\n",
    "    n_samples = X.shape[0]\n",
    "    split_size = n_samples // cut\n",
    "\n",
    "    for i in tqdm(range(cut)):\n",
    "        start_test = i * split_size\n",
    "        end_test = (i + 1) * split_size if i < cut - 1 else n_samples\n",
    "\n",
    "        X_test = X[start_test:end_test]\n",
    "        y_test = y[start_test:end_test]\n",
    "\n",
    "        indices = list(range(0, start_test)) + list(range(end_test, n_samples))\n",
    "        X_train = X.iloc[indices]\n",
    "        y_train = y.iloc[indices]\n",
    "\n",
    "        lasso_cv = LassoCV(cv=10, random_state=42, max_iter=10000, n_jobs=-1).fit(X_train, y_train)\n",
    "        best_alpha = lasso_cv.alpha_\n",
    "        lasso = Lasso(alpha=best_alpha)\n",
    "        lasso.fit(X_train, y_train)\n",
    "\n",
    "        selected_features = np.where(lasso.coef_ != 0)[0]\n",
    "\n",
    "        count = 0\n",
    "        while len(selected_features) < top:\n",
    "            count += 1\n",
    "            lasso = Lasso(alpha=best_alpha*(0.5**count))\n",
    "            lasso.fit(X_train, y_train)\n",
    "            selected_features = np.where(lasso.coef_ != 0)[0]\n",
    "\n",
    "        selected_feature_names = X_train.columns[selected_features]\n",
    "        selected_features_list.append(selected_feature_names)\n",
    "        total_features_list.append(len(selected_feature_names))\n",
    "\n",
    "    for i, features in enumerate(selected_features_list):\n",
    "        print(f'Total: {total_features_list[i]}')\n",
    "        print(f\"Fold {i+1}: {features.tolist()}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
